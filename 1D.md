# ProSetting Progressive Training System

## ğŸ¯ Project Overview

ProSetting is a progressive reinforcement learning training system that supports both VERL PPO and TRL DPO training frameworks. The system enables iterative training of Solver models with complete modular architecture and semi-automated control, implementing weight transfer between rounds and data augmentation.

## ğŸ—ï¸ System Architecture

### Core Components
- **Solver Model**: Mathematical problem-solving model supporting VERL PPO and TRL DPO training
- **Teacher Models**: Including Teacher1 (grading) and Teacher2 (question enhancement)
- **Training Frameworks**:
  - **VERL PPO**: Original reinforcement learning training engine
  - **TRL DPO**: Direct preference optimization training framework (recommended)

### Training Flow

#### VERL PPO Flow
```
Original Questions â†’ Solver Solving â†’ Teacher1 Grading â†’ Teacher2 Enhancement â†’ Next Round Training
       â†“                â†“                â†“                  â†“                    â†“
Trajectory Collection  Reward Calculation  Question Generation  Data Preparation  Weight Update
```

#### TRL DPO Flow (Recommended)
```
Original Questions â†’ Solver Solving â†’ Teacher1 Grading â†’ DPO Triplets â†’ Next Round Training
       â†“                â†“                â†“                â†“               â†“
Trajectory Collection  Grading Results   Cartesian Product  Parquet Format  Weight Update
```

## ğŸ“ Project Structure

```
ProSetting/
â”œâ”€â”€ run_training.py           # Unified training launcher script
â”œâ”€â”€ auto_trainer.py           # Fully automated TRL training script (recommended)
â”œâ”€â”€ semi_auto_trainer_trl.py  # Semi-automated TRL training script
â”œâ”€â”€ collectors/               # Data collection modules
â”‚   â”œâ”€â”€ trajectory_collector.py  # Trajectory collector
â”‚   â””â”€â”€ data_normalizer.py       # Data normalizer
â”œâ”€â”€ processors/               # Data processing modules
â”‚   â”œâ”€â”€ reward_calculator.py     # Reward calculator
â”‚   â”œâ”€â”€ question_enhancer.py     # Question enhancer
â”‚   â””â”€â”€ solver_data_processor.py # Solver data processor
â”œâ”€â”€ datasets/                 # Dataset modules
â”‚   â”œâ”€â”€ dpo_data_converter.py    # DPO data converter
â”‚   â”œâ”€â”€ dpo_data_generator.py    # DPO data generator
â”‚   â””â”€â”€ data_saver.py            # Data saver
â”œâ”€â”€ trainers/                 # Training modules
â”‚   â”œâ”€â”€ trl_trainer.py           # TRL trainer
â”‚   â””â”€â”€ gpu_manager.py           # GPU manager
â”œâ”€â”€ managers/                 # Management modules
â”‚   â”œâ”€â”€ round_controller.py      # Round controller
â”‚   â””â”€â”€ question_manager.py      # Question manager
â”œâ”€â”€ core/                     # Core modules
â”‚   â””â”€â”€ state_manager.py         # State manager
â”œâ”€â”€ Teacher_Model/           # Teacher model clients
â”œâ”€â”€ utils/                   # Utility scripts
â”œâ”€â”€ DataSet/                 # Dataset files
â”œâ”€â”€ .env.example             # Environment configuration example
â””â”€â”€ requirements.txt          # Project dependencies
```

## ğŸš€ Quick Start

### Environment Setup

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure environment variables
cp .env.example .env
# Edit .env file and set the following key parameters:
# SOLVER_MODEL_PATH=/path/to/solver/model
# QUESTIONS_FILE=/path/to/questions.json
# WORKSPACE_DIR=/path/to/workspace
# TRL_NUM_PROCESSES=8
# TEACHER_BASE_URL=http://your-teacher-api

# 3. Verify environment
python utils/status_checker.py --quick
```

### Running Methods

#### 1. Unified Launcher Script (Recommended)
```bash
cd /home/project/ProSetting

# Fully automated training (default)
python run_training.py

# Semi-automated training
python run_training.py --mode semi
```

#### 2. Direct Training Scripts
```bash
cd /home/project/ProSetting

# Fully automated TRL training (recommended)
python auto_trainer.py

# Semi-automated TRL training
python semi_auto_trainer_trl.py
```

#### 3. System Testing and Status Check
```bash
# Quick test of entire system logic
python utils/test_runner.py

# Check system status and configuration
python utils/status_checker.py

# Quick status check
python utils/status_checker.py --quick
```

## ğŸ® Training Mode Usage

### Fully Automated TRL Training (Recommended)

Fully automated TRL training is an unattended version developed based on semi-automated scripts, featuring:

- **Complete Automation**: No manual intervention required, automatically completes all round training
- **Smart Retry**: Supports automatic retry mechanism with configurable retry count and intervals
- **Error Recovery**: Option to skip or stop when training fails
- **Checkpoint Resume**: Supports recovery from any stage
- **Resource Management**: Automatic GPU memory cleanup and model resource management
- **Detailed Logging**: Complete training process records and final reports
- **Signal Handling**: Supports graceful shutdown (Ctrl+C)

#### Configuration Parameters
```bash
# Fully automated configuration in .env file
AUTO_RETRY_ENABLED=true          # Enable automatic retry
AUTO_CONTINUE_ON_FAILURE=false   # Whether to continue next round on failure
CHECKPOINT_INTERVAL=1            # Checkpoint save interval
```

#### Usage
```bash
# Start fully automated training
python auto_trainer.py

# Run in background (recommended for long training)
nohup python auto_trainer.py > auto_training.log 2>&1 &

# View training progress
tail -f auto_training.log
```

#### Training Report
Detailed report generated after training completion:
- Total training duration and round statistics
- Success/failure round details
- Total number of questions processed
- Error information summary
- Final training status

## ğŸ® Semi-Automated Training Usage

### TRL DPO Training (Recommended)

TRL DPO training uses direct preference optimization, more stable and efficient than PPO training:

- **Data Format**: Convert grading results to DPO triplets (prompt, chosen, rejected)
- **Training Method**: Distributed training using accelerate + TRL
- **Data Storage**: Parquet format for efficient reading
- **Weight Transfer**: Automatic model weight management between rounds
- **Training Schedule**: First 2 rounds no training (data accumulation), training starts from round 3
- **Checkpoint Resume**: Supports 5 fine-grained stage checkpoint protection and recovery

### Interactive Commands
- **`c`** - Continue executing current round training
- **`s`** - View detailed status information  
- **`q`** - Exit training system
- **`r`** - Retry current round (available when training fails)

### Training Stages
TRL training includes the following 5 stages with checkpoint resume support:
1. **Data Collection**: Multi-GPU parallel solver trajectory collection
2. **Data Grading**: Teacher1 batch grading with 32 concurrent processing
3. **DPO Conversion**: Convert grading results to DPO triplets, save as parquet format
4. **TRL Training**: Distributed training using accelerate + TRL (skip first 2 rounds)
5. **Next Round Preparation**: Build next round question pool with enhanced questions and failed replays

### Training Flow
```
ğŸš€ ========== Semi-Automated Training System Initialization ==========
âœ… Initialization Complete
ğŸ“Š Configuration Info:
   - Total Rounds: 5
   - Save Rounds: [3, 4, 5]
   - Workspace: /tmp/prosetting_workspace

ğŸ“Š Current Status:
   - Current Round: 1
   - Completed Rounds: 0
   - Total Rounds: 5
   - Can Continue: Yes

Preparing to execute Round 1 training
Please select operation [c]continue [s]status [q]quit: c
```

## ğŸ“Š Data Management

### File Structure
```
/tmp/prosetting_workspace/
â”œâ”€â”€ training_state.json                    # Training state
â”œâ”€â”€ training_config.json                   # Training configuration
â”œâ”€â”€ data/                                  # Round data
â”‚   â”œâ”€â”€ round_01_questions.json           # Question data
â”‚   â”œâ”€â”€ round_01_trajectories.json        # Trajectory data
â”‚   â”œâ”€â”€ round_01_rewards.json             # Reward data
â”‚   â”œâ”€â”€ round_01_enhanced_questions.json  # Enhanced questions
â”‚   â”œâ”€â”€ round_01_solver_training.json     # Solver training data
â”‚   â”œâ”€â”€ round_01_generator_training.json  # Generator training data
â”‚   â””â”€â”€ round_01_results.json             # Round results
â”œâ”€â”€ training_data/                         # Training files
â”‚   â””â”€â”€ solver_r1_20250122_143052.json    # VERL training data
â”œâ”€â”€ datasets/                              # TRL datasets
â”‚   â””â”€â”€ round_01/                          # Round dataset
â”‚       â”œâ”€â”€ train.parquet                  # Training set (90%)
â”‚       â”œâ”€â”€ validation.parquet             # Validation set (10%)
â”‚       â””â”€â”€ dataset_info.json             # Dataset information
â”œâ”€â”€ training_results/                      # Training results
â”‚   â””â”€â”€ trl_r1_result.json                # TRL training result records
â”œâ”€â”€ checkpoints/                           # Merged weights
â”‚   â””â”€â”€ merged_solver_r1/                 # Merged model weights
â””â”€â”€ results/                               # Final results
```

### Data Flow
```
Rounds 1-3: Data accumulation phase (no training)
Round 1: Original questions(120) â†’ Trajectory collection â†’ Grading â†’ DPO conversion â†’ Data saving
Round 2: Enhanced question pool(~200) â†’ Trajectory collection â†’ Grading â†’ DPO conversion â†’ Data saving  
Round 3: Enhanced question pool(~250) â†’ Trajectory collection â†’ Grading â†’ DPO conversion â†’ Data saving

Round 4+: Training phase
Round 4: Enhanced question pool â†’ Data processing â†’ TRL training â†’ Weight update
Round 5: Enhanced question pool + Previous round weights â†’ TRL training â†’ Weight update
  â†“ And so on...
```

## âš™ï¸ Configuration

### Default Configuration
```python
{
    "max_rounds": 5,                    # Total training rounds
    "save_rounds": [3, 4, 5],          # Checkpoint save rounds (deprecated)
    "attempts_per_question": 8,         # Attempts per question
    "physical_solver_gpu": "4",         # Solver model GPU
    "physical_grpo_gpu": "0,1,2,3,4,5,6,7",  # Training GPUs
    "training_framework": "TRL_DPO",   # Training framework
    "trl_num_processes": 8,            # TRL training processes
    "trl_mixed_precision": "bf16"      # Mixed precision training
}
```

### Model Paths
- **Solver Model**: `/home/jiaozhengbo.jzb/verl-new/ProSetting/Model/Qwen3-8B`
- **Generator Model**: `/home/jiaozhengbo.jzb/verl-new/ProSetting/Model/Qwen2.5-32B`
- **Question Data**: `/home/jiaozhengbo.jzb/data/rlhf/gsm8k/merge_dataseed.json`

## ğŸ”§ Core Features

### 1. Inter-Round Weight Transfer
- Round 1 uses original weights for training
- Round 2+ automatically loads previous round training results
- Supports FSDP distributed weight auto-merging

### 2. Progressive Question Enhancement
- Teacher1 batch grading for reward calculation
- Teacher2 generates enhanced questions based on error analysis
- Failed question replay + 20% random replay mechanism

### 3. Data Persistence
- All training data permanently saved
- Standardized file naming conventions
- Training state recovery support

### 4. Modular Architecture
- Separated data collection, processing, training, and management modules
- Independent testing and maintenance support
- Complete error handling mechanisms

## ğŸ¯ Training Strategy

### Round Configuration
- **Total Rounds**: 5 rounds (configurable)
- **Training Strategy**: First 3 rounds data accumulation, training starts from round 4
- **Weight Strategy**: Round 4 uses original weights, subsequent rounds use previous round output weights
- **Save Strategy**: Automatic model weight saving after each training round

### Question Pool Management
- **Round 1**: Original questions (quantity depends on data file)
- **Round 2+**: Enhanced questions + failed replays + random replays
- **Question Growth**: Question pool gradually expands each round
- **Enhancement Strategy**: Teacher2 generates enhanced questions based on error analysis, 32 concurrent processing
- **Replay Mechanism**: 100% failed question replay, full non-failed question replay

### GPU Allocation
- **Solver Collection**: Configurable GPU (default GPU 4)
- **VERL Training**: GPU 0-7 (8-card parallel, traditional method)
- **TRL Training**: GPU 0-7 (8-card parallel, using accelerate)
- **Memory Management**: Automatic cleanup and release between stages
- **Parallel Strategy**:
  - Data collection: Multi-GPU parallel with intelligent task allocation
  - Grading processing: 32 concurrent Teacher API calls
  - Question enhancement: 32 concurrent Teacher2 processing

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **Model path does not exist**
   ```bash
   export SOLVER_MODEL_PATH="/correct/path/to/model"
   ```

2. **Insufficient GPU memory**
   - Check GPU usage: `nvidia-smi`
   - Adjust batch_size or reduce parallelism

3. **Checkpoint merge failure**
   - Check checkpoint directory permissions
   - Confirm FSDP weight files are complete

4. **Training interruption recovery**
   ```bash
   # Fully automated training recovery
   python auto_trainer.py
   
   # Semi-automated training recovery
   python semi_auto_trainer_trl.py
   
   # Check recovery status
   python utils/status_checker.py
   ```

### Log Files
- **VERL Training Log**: `/tmp/semi_auto_trainer.log`
- **TRL Training Log**: `/tmp/trl_trainer.log`
- **Fully Automated Training Log**: `/tmp/auto_trainer.log`
- **Training Output**: Real-time console output
- **State Files**: `{WORKSPACE_DIR}/training_state.json`
- **Round Progress**: `{WORKSPACE_DIR}/round_XX_progress.json`
- **Training Results**: `{WORKSPACE_DIR}/training_results/`
- **Training Summary**: `{WORKSPACE_DIR}/auto_training_summary.json`
- **Checkpoint Files**: `{WORKSPACE_DIR}/checkpoint_round_X.json`

## ğŸ“ˆ Performance Monitoring

### Training Metrics
- **Reward Changes**: Average reward trends per round
- **Question Quality**: Diversity of enhanced questions
- **Model Convergence**: Training loss and gradient changes

### Resource Usage
- **GPU Utilization**: 8-card training efficiency
- **Memory Usage**: Model and data memory consumption
- **Storage Space**: Checkpoint and data file sizes

## ğŸ¤ Development Guide

### Adding New Modules
1. Create new files in appropriate directories
2. Implement standard interfaces and error handling
3. Update corresponding `__init__.py` exports
4. Add unit tests

### Custom Training Strategies
1. Modify `RoundController` configuration
2. Adjust question pool building logic
3. Customize reward calculation functions

### Extending Data Formats
1. Update `StateManager` file naming
2. Modify data save and load logic
3. Ensure backward compatibility

## ğŸ“„ License

This project follows internal use license, for research and development only.

## ğŸ™‹ Support

For questions or suggestions, please contact the development team or check project documentation.

---

**Note**: This system has completed modular refactoring, reorganizing the original `modules` directory into clearer functional modules:
- `collectors/` - Data collection related
- `processors/` - Data processing related  
- `datasets/` - Dataset management related
- `trainers/` - Training execution related
- `managers/` - Round and question management related
- `core/` - Core state management

This new directory structure is more intuitive and easier to understand and maintain.