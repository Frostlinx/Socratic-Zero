# ProSetting Progressive Training System

## 🎯 Project Overview

ProSetting is a progressive reinforcement learning training system that supports both VERL PPO and TRL DPO training frameworks. The system enables iterative training of Solver models with complete modular architecture and semi-automated control, implementing weight transfer between rounds and data augmentation.

## 🏗️ System Architecture

### Core Components
- **Solver Model**: Mathematical problem-solving model supporting VERL PPO and TRL DPO training
- **Teacher Models**: Including Teacher1 (grading) and Teacher2 (question enhancement)
- **Training Frameworks**:
  - **VERL PPO**: Original reinforcement learning training engine
  - **TRL DPO**: Direct preference optimization training framework (recommended)

### Training Flow

#### VERL PPO Flow
```
Original Questions → Solver Solving → Teacher1 Grading → Teacher2 Enhancement → Next Round Training
       ↓                ↓                ↓                  ↓                    ↓
Trajectory Collection  Reward Calculation  Question Generation  Data Preparation  Weight Update
```

#### TRL DPO Flow (Recommended)
```
Original Questions → Solver Solving → Teacher1 Grading → DPO Triplets → Next Round Training
       ↓                ↓                ↓                ↓               ↓
Trajectory Collection  Grading Results   Cartesian Product  Parquet Format  Weight Update
```

## 📁 Project Structure

```
ProSetting/
├── run_training.py           # Unified training launcher script
├── auto_trainer.py           # Fully automated TRL training script (recommended)
├── semi_auto_trainer_trl.py  # Semi-automated TRL training script
├── collectors/               # Data collection modules
│   ├── trajectory_collector.py  # Trajectory collector
│   └── data_normalizer.py       # Data normalizer
├── processors/               # Data processing modules
│   ├── reward_calculator.py     # Reward calculator
│   ├── question_enhancer.py     # Question enhancer
│   └── solver_data_processor.py # Solver data processor
├── datasets/                 # Dataset modules
│   ├── dpo_data_converter.py    # DPO data converter
│   ├── dpo_data_generator.py    # DPO data generator
│   └── data_saver.py            # Data saver
├── trainers/                 # Training modules
│   ├── trl_trainer.py           # TRL trainer
│   └── gpu_manager.py           # GPU manager
├── managers/                 # Management modules
│   ├── round_controller.py      # Round controller
│   └── question_manager.py      # Question manager
├── core/                     # Core modules
│   └── state_manager.py         # State manager
├── Teacher_Model/           # Teacher model clients
├── utils/                   # Utility scripts
├── DataSet/                 # Dataset files
├── .env.example             # Environment configuration example
└── requirements.txt          # Project dependencies
```

## 🚀 Quick Start

### Environment Setup

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure environment variables
cp .env.example .env
# Edit .env file and set the following key parameters:
# SOLVER_MODEL_PATH=/path/to/solver/model
# QUESTIONS_FILE=/path/to/questions.json
# WORKSPACE_DIR=/path/to/workspace
# TRL_NUM_PROCESSES=8
# TEACHER_BASE_URL=http://your-teacher-api

# 3. Verify environment
python utils/status_checker.py --quick
```

### Running Methods

#### 1. Unified Launcher Script (Recommended)
```bash
cd /home/project/ProSetting

# Fully automated training (default)
python run_training.py

# Semi-automated training
python run_training.py --mode semi
```

#### 2. Direct Training Scripts
```bash
cd /home/project/ProSetting

# Fully automated TRL training (recommended)
python auto_trainer.py

# Semi-automated TRL training
python semi_auto_trainer_trl.py
```

#### 3. System Testing and Status Check
```bash
# Quick test of entire system logic
python utils/test_runner.py

# Check system status and configuration
python utils/status_checker.py

# Quick status check
python utils/status_checker.py --quick
```

## 🎮 Training Mode Usage

### Fully Automated TRL Training (Recommended)

Fully automated TRL training is an unattended version developed based on semi-automated scripts, featuring:

- **Complete Automation**: No manual intervention required, automatically completes all round training
- **Smart Retry**: Supports automatic retry mechanism with configurable retry count and intervals
- **Error Recovery**: Option to skip or stop when training fails
- **Checkpoint Resume**: Supports recovery from any stage
- **Resource Management**: Automatic GPU memory cleanup and model resource management
- **Detailed Logging**: Complete training process records and final reports
- **Signal Handling**: Supports graceful shutdown (Ctrl+C)

#### Configuration Parameters
```bash
# Fully automated configuration in .env file
AUTO_RETRY_ENABLED=true          # Enable automatic retry
AUTO_CONTINUE_ON_FAILURE=false   # Whether to continue next round on failure
CHECKPOINT_INTERVAL=1            # Checkpoint save interval
```

#### Usage
```bash
# Start fully automated training
python auto_trainer.py

# Run in background (recommended for long training)
nohup python auto_trainer.py > auto_training.log 2>&1 &

# View training progress
tail -f auto_training.log
```

#### Training Report
Detailed report generated after training completion:
- Total training duration and round statistics
- Success/failure round details
- Total number of questions processed
- Error information summary
- Final training status

## 🎮 Semi-Automated Training Usage

### TRL DPO Training (Recommended)

TRL DPO training uses direct preference optimization, more stable and efficient than PPO training:

- **Data Format**: Convert grading results to DPO triplets (prompt, chosen, rejected)
- **Training Method**: Distributed training using accelerate + TRL
- **Data Storage**: Parquet format for efficient reading
- **Weight Transfer**: Automatic model weight management between rounds
- **Training Schedule**: First 2 rounds no training (data accumulation), training starts from round 3
- **Checkpoint Resume**: Supports 5 fine-grained stage checkpoint protection and recovery

### Interactive Commands
- **`c`** - Continue executing current round training
- **`s`** - View detailed status information  
- **`q`** - Exit training system
- **`r`** - Retry current round (available when training fails)

### Training Stages
TRL training includes the following 5 stages with checkpoint resume support:
1. **Data Collection**: Multi-GPU parallel solver trajectory collection
2. **Data Grading**: Teacher1 batch grading with 32 concurrent processing
3. **DPO Conversion**: Convert grading results to DPO triplets, save as parquet format
4. **TRL Training**: Distributed training using accelerate + TRL (skip first 2 rounds)
5. **Next Round Preparation**: Build next round question pool with enhanced questions and failed replays

### Training Flow
```
🚀 ========== Semi-Automated Training System Initialization ==========
✅ Initialization Complete
📊 Configuration Info:
   - Total Rounds: 5
   - Save Rounds: [3, 4, 5]
   - Workspace: /tmp/prosetting_workspace

📊 Current Status:
   - Current Round: 1
   - Completed Rounds: 0
   - Total Rounds: 5
   - Can Continue: Yes

Preparing to execute Round 1 training
Please select operation [c]continue [s]status [q]quit: c
```

## 📊 Data Management

### File Structure
```
/tmp/prosetting_workspace/
├── training_state.json                    # Training state
├── training_config.json                   # Training configuration
├── data/                                  # Round data
│   ├── round_01_questions.json           # Question data
│   ├── round_01_trajectories.json        # Trajectory data
│   ├── round_01_rewards.json             # Reward data
│   ├── round_01_enhanced_questions.json  # Enhanced questions
│   ├── round_01_solver_training.json     # Solver training data
│   ├── round_01_generator_training.json  # Generator training data
│   └── round_01_results.json             # Round results
├── training_data/                         # Training files
│   └── solver_r1_20250122_143052.json    # VERL training data
├── datasets/                              # TRL datasets
│   └── round_01/                          # Round dataset
│       ├── train.parquet                  # Training set (90%)
│       ├── validation.parquet             # Validation set (10%)
│       └── dataset_info.json             # Dataset information
├── training_results/                      # Training results
│   └── trl_r1_result.json                # TRL training result records
├── checkpoints/                           # Merged weights
│   └── merged_solver_r1/                 # Merged model weights
└── results/                               # Final results
```

### Data Flow
```
Rounds 1-3: Data accumulation phase (no training)
Round 1: Original questions(120) → Trajectory collection → Grading → DPO conversion → Data saving
Round 2: Enhanced question pool(~200) → Trajectory collection → Grading → DPO conversion → Data saving  
Round 3: Enhanced question pool(~250) → Trajectory collection → Grading → DPO conversion → Data saving

Round 4+: Training phase
Round 4: Enhanced question pool → Data processing → TRL training → Weight update
Round 5: Enhanced question pool + Previous round weights → TRL training → Weight update
  ↓ And so on...
```

## ⚙️ Configuration

### Default Configuration
```python
{
    "max_rounds": 5,                    # Total training rounds
    "save_rounds": [3, 4, 5],          # Checkpoint save rounds (deprecated)
    "attempts_per_question": 8,         # Attempts per question
    "physical_solver_gpu": "4",         # Solver model GPU
    "physical_grpo_gpu": "0,1,2,3,4,5,6,7",  # Training GPUs
    "training_framework": "TRL_DPO",   # Training framework
    "trl_num_processes": 8,            # TRL training processes
    "trl_mixed_precision": "bf16"      # Mixed precision training
}
```

### Model Paths
- **Solver Model**: `/home/jiaozhengbo.jzb/verl-new/ProSetting/Model/Qwen3-8B`
- **Generator Model**: `/home/jiaozhengbo.jzb/verl-new/ProSetting/Model/Qwen2.5-32B`
- **Question Data**: `/home/jiaozhengbo.jzb/data/rlhf/gsm8k/merge_dataseed.json`

## 🔧 Core Features

### 1. Inter-Round Weight Transfer
- Round 1 uses original weights for training
- Round 2+ automatically loads previous round training results
- Supports FSDP distributed weight auto-merging

### 2. Progressive Question Enhancement
- Teacher1 batch grading for reward calculation
- Teacher2 generates enhanced questions based on error analysis
- Failed question replay + 20% random replay mechanism

### 3. Data Persistence
- All training data permanently saved
- Standardized file naming conventions
- Training state recovery support

### 4. Modular Architecture
- Separated data collection, processing, training, and management modules
- Independent testing and maintenance support
- Complete error handling mechanisms

## 🎯 Training Strategy

### Round Configuration
- **Total Rounds**: 5 rounds (configurable)
- **Training Strategy**: First 3 rounds data accumulation, training starts from round 4
- **Weight Strategy**: Round 4 uses original weights, subsequent rounds use previous round output weights
- **Save Strategy**: Automatic model weight saving after each training round

### Question Pool Management
- **Round 1**: Original questions (quantity depends on data file)
- **Round 2+**: Enhanced questions + failed replays + random replays
- **Question Growth**: Question pool gradually expands each round
- **Enhancement Strategy**: Teacher2 generates enhanced questions based on error analysis, 32 concurrent processing
- **Replay Mechanism**: 100% failed question replay, full non-failed question replay

### GPU Allocation
- **Solver Collection**: Configurable GPU (default GPU 4)
- **VERL Training**: GPU 0-7 (8-card parallel, traditional method)
- **TRL Training**: GPU 0-7 (8-card parallel, using accelerate)
- **Memory Management**: Automatic cleanup and release between stages
- **Parallel Strategy**:
  - Data collection: Multi-GPU parallel with intelligent task allocation
  - Grading processing: 32 concurrent Teacher API calls
  - Question enhancement: 32 concurrent Teacher2 processing

## 🛠️ Troubleshooting

### Common Issues

1. **Model path does not exist**
   ```bash
   export SOLVER_MODEL_PATH="/correct/path/to/model"
   ```

2. **Insufficient GPU memory**
   - Check GPU usage: `nvidia-smi`
   - Adjust batch_size or reduce parallelism

3. **Checkpoint merge failure**
   - Check checkpoint directory permissions
   - Confirm FSDP weight files are complete

4. **Training interruption recovery**
   ```bash
   # Fully automated training recovery
   python auto_trainer.py
   
   # Semi-automated training recovery
   python semi_auto_trainer_trl.py
   
   # Check recovery status
   python utils/status_checker.py
   ```

### Log Files
- **VERL Training Log**: `/tmp/semi_auto_trainer.log`
- **TRL Training Log**: `/tmp/trl_trainer.log`
- **Fully Automated Training Log**: `/tmp/auto_trainer.log`
- **Training Output**: Real-time console output
- **State Files**: `{WORKSPACE_DIR}/training_state.json`
- **Round Progress**: `{WORKSPACE_DIR}/round_XX_progress.json`
- **Training Results**: `{WORKSPACE_DIR}/training_results/`
- **Training Summary**: `{WORKSPACE_DIR}/auto_training_summary.json`
- **Checkpoint Files**: `{WORKSPACE_DIR}/checkpoint_round_X.json`

## 📈 Performance Monitoring

### Training Metrics
- **Reward Changes**: Average reward trends per round
- **Question Quality**: Diversity of enhanced questions
- **Model Convergence**: Training loss and gradient changes

### Resource Usage
- **GPU Utilization**: 8-card training efficiency
- **Memory Usage**: Model and data memory consumption
- **Storage Space**: Checkpoint and data file sizes

## 🤝 Development Guide

### Adding New Modules
1. Create new files in appropriate directories
2. Implement standard interfaces and error handling
3. Update corresponding `__init__.py` exports
4. Add unit tests

### Custom Training Strategies
1. Modify `RoundController` configuration
2. Adjust question pool building logic
3. Customize reward calculation functions

### Extending Data Formats
1. Update `StateManager` file naming
2. Modify data save and load logic
3. Ensure backward compatibility

## 📄 License

This project follows internal use license, for research and development only.

## 🙋 Support

For questions or suggestions, please contact the development team or check project documentation.

---

**Note**: This system has completed modular refactoring, reorganizing the original `modules` directory into clearer functional modules:
- `collectors/` - Data collection related
- `processors/` - Data processing related  
- `datasets/` - Dataset management related
- `trainers/` - Training execution related
- `managers/` - Round and question management related
- `core/` - Core state management

This new directory structure is more intuitive and easier to understand and maintain.