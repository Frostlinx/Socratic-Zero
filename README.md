<div align="center">
  <img src="scripts/method.pdf" alt="Socratic-Zero Framework" width="800"/>
  
  # Socratic-Zero
  
  **Bootstrapping Reasoning via Data-Free Agent Co-evolution**
  
  [![arXiv](https://img.shields.io/badge/arXiv-2509.24726-b31b1b.svg)](http://arxiv.org/abs/2509.24726)
  [![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
  [![License](https://img.shields.io/badge/License-Research-green.svg)](#license)
  
</div>

---

## ğŸ¯ Overview

<p align="center">
  <img src="scripts/pipeline.pdf" alt="Socratic-Zero Pipeline" width="900"/>
  <br>
  <em>The Socratic-Zero Framework: Multi-Agent Co-evolutionary System Pipeline</em>
</p>

ProSetting is the implementation of the Socratic-Zero framework - a progressive reinforcement learning training system that enables iterative training of mathematical reasoning models through co-evolution of three agents: **Solver**, **Teacher**, and **Generator**. Starting from only 100 seed questions, our approach achieves significant improvements without relying on massive external datasets.

### ğŸ† Key Results

<div align="center">
  <img src="scripts/comparison (1).pdf" alt="Performance Comparison" width="800"/>
  <br>
  <em>Solver and Generator Performance Comparison</em>
</div>

- **ğŸ§  Socratic-Solver-8B**: Achieves **+20.2 percentage points** average improvement across seven mathematical reasoning benchmarks
- **ğŸ­ Socratic-Generator-32B**: Produces synthetic data enabling student models to outperform commercial LLMs including GPT-5, Gemini-2.5-Pro, and Claude-4.1-Opus
- **ğŸ”„ Cross-Architecture**: Consistent improvements on Qwen3 and GLM4 model families

## ğŸ—ï¸ Architecture

### Core Components

<div align="center">
  
| Component | Role | Description |
|-----------|------|-------------|
| ğŸ§  **Solver Model** | Reasoning Agent | Mathematical reasoning model that learns from preference feedback |
| ğŸ‘¨â€ğŸ« **Teacher Model** | Oracle & Evaluator | Fixed oracle providing evaluation and strategic problem generation |
| ğŸ­ **Generator Model** | Curriculum Designer | Learns to distill Teacher's curriculum design strategy |
| âš™ï¸ **Training Frameworks** | Execution Engine | Supports both VERL PPO and TRL DPO (recommended) |

</div>

### Training Flow

```mermaid
graph LR
    A[ğŸ“š Questions] --> B[ğŸ§  Solver]
    B --> C[ğŸ‘¨â€ğŸ« Teacher]
    C --> D[ğŸ“Š DPO Triplets]
    D --> E[ğŸ”„ Next Round]
    
    B -.-> F[ğŸ“ Collection]
    C -.-> G[âœ… Grading]
    D -.-> H[ğŸ”€ Cartesian]
    E -.-> I[ğŸ’¾ Parquet]
    I -.-> J[âš–ï¸ Weight Update]
    
    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style E fill:#fce4ec
```

## ğŸ“ Project Structure

```
ProSetting/
â”œâ”€â”€ ğŸ“œ scripts/
â”‚   â”œâ”€â”€ run_training.py           # ğŸš€ Unified training launcher
â”‚   â”œâ”€â”€ auto_trainer.py           # ğŸ¤– Fully automated training (recommended)
â”‚   â”œâ”€â”€ semi_auto_trainer.py      # ğŸ® Interactive training
â”‚   â”œâ”€â”€ method.pdf               # ğŸ–¼ï¸ Framework overview figure
â”‚   â”œâ”€â”€ pipeline.pdf             # ğŸ”„ Detailed pipeline diagram
â”‚   â””â”€â”€ comparison (1).pdf       # ğŸ“Š Results comparison chart
â”œâ”€â”€ ğŸ—‚ï¸ collectors/                # Data collection modules
â”‚   â”œâ”€â”€ trajectory_collector.py   # ğŸ”„ Multi-GPU trajectory generation
â”‚   â””â”€â”€ data_normalizer.py        # ğŸ“ Data standardization
â”œâ”€â”€ âš™ï¸ processors/                # Data processing modules
â”‚   â”œâ”€â”€ reward_calculator.py      # ğŸ† Teacher-based reward computation
â”‚   â”œâ”€â”€ question_enhancer.py      # ğŸ“ˆ Progressive question generation
â”‚   â””â”€â”€ solver_data_processor.py  # ğŸ”§ Training data preparation
â”œâ”€â”€ ğŸ’¾ datasets/                  # Dataset management
â”‚   â”œâ”€â”€ dpo_data_converter.py     # ğŸ”„ DPO format conversion
â”‚   â””â”€â”€ data_saver.py             # ğŸ’¾ Data persistence
â”œâ”€â”€ ğŸ‹ï¸ trainers/                 # Training execution
â”‚   â”œâ”€â”€ trl_trainer.py            # ğŸ¯ TRL-based training
â”‚   â””â”€â”€ gpu_manager.py            # ğŸ–¥ï¸ Resource management
â”œâ”€â”€ ğŸ›ï¸ managers/                 # System management
â”‚   â”œâ”€â”€ round_controller.py       # ğŸ”„ Multi-round coordination
â”‚   â””â”€â”€ question_manager.py       # â“ Question pool management
â””â”€â”€ ğŸ§  core/                     # Core utilities
    â””â”€â”€ state_manager.py          # ğŸ’¾ Training state persistence
```

## ğŸš€ Quick Start

### Environment Setup

```bash
# 1. Clone the repository
git clone https://github.com/Frostlinx/Socratic-Zero.git
cd Socratic-Zero

# 2. Install dependencies
pip install -r requirements.txt

# 3. Configure environment variables
cp .env.example .env
# Edit .env file with key parameters:
# SOLVER_MODEL_PATH=/path/to/solver/model
# QUESTIONS_FILE=/path/to/questions.json
# WORKSPACE_DIR=/path/to/workspace
# TRL_NUM_PROCESSES=8
# TEACHER_BASE_URL=http://your-teacher-api

# 4. Verify environment
python utils/status_checker.py --quick
```

### ğŸ® Running Training

#### 1. Unified Launcher (Recommended)
```bash
cd /home/project/ProSetting

# ğŸ¤– Fully automated training (default)
python scripts/run_training.py

# ğŸ® Semi-automated training
python scripts/run_training.py --mode semi
```

#### 2. Direct Training Scripts
```bash
cd /home/project/ProSetting

# ğŸ¤– Fully automated TRL training (recommended)
python scripts/auto_trainer.py

# ğŸ® Semi-automated TRL training
python scripts/semi_auto_trainer.py
```

#### 3. System Testing
```bash
# ğŸ§ª Quick system logic test
python utils/test_runner.py

# ğŸ“Š System status check
python utils/status_checker.py

# âš¡ Quick status check
python utils/status_checker.py --quick
```

## âš™ï¸ Training Features

### ğŸ¤– Fully Automated Training (Recommended)

<details>
<summary>ğŸ“‹ Click to expand features</summary>

- **ğŸ”„ Complete Automation**: No manual intervention required
- **ğŸ” Smart Retry**: Configurable retry mechanism with intervals
- **ğŸ› ï¸ Error Recovery**: Option to skip or stop on training failures
- **ğŸ’¾ Checkpoint Recovery**: Resume from any stage
- **ğŸ–¥ï¸ Resource Management**: Automatic GPU memory cleanup
- **ğŸ“ Detailed Logging**: Complete training process records and final reports
- **âš¡ Signal Handling**: Graceful shutdown support (Ctrl+C)

</details>

## âš™ï¸ Configuration

### Default Configuration
```python
{
    "max_rounds": 5,                    # ğŸ”„ Total training rounds
    "save_rounds": [3, 4, 5],          # ğŸ’¾ Checkpoint save rounds
    "attempts_per_question": 8,         # ğŸ¯ Attempts per question
    "physical_solver_gpu": "4",         # ğŸ–¥ï¸ Solver model GPU
    "physical_grpo_gpu": "0,1,2,3,4,5,6,7",  # ğŸ–¥ï¸ Training GPUs
    "training_framework": "TRL_DPO",   # ğŸ‹ï¸ Training framework
    "trl_num_processes": 8,            # âš¡ TRL training processes
    "trl_mixed_precision": "bf16"      # ğŸ¯ Mixed precision training
}
```

### ğŸ¯ Model Paths
- **ğŸ§  Solver Model**: Configure in `SOLVER_MODEL_PATH`
- **ğŸ­ Generator Model**: Configure in `GENERATOR_MODEL_PATH`  
- **ğŸ“š Question Data**: Configure in `QUESTIONS_FILE`
- **ğŸ‘¨â€ğŸ« Teacher API**: Configure in `TEACHER_BASE_URL`

## ğŸŒŸ Core Features

### 1. ğŸ“ˆ Progressive Training Strategy
- **ğŸ”„ Rounds 1-2**: Data accumulation phase without model updates
- **ğŸš€ Round 3+**: Active training with progressive weight transfer
- **ğŸ“š Question Pool Evolution**: Systematic expansion through teacher-guided enhancement
- **ğŸ› ï¸ Failure Recovery**: Automatic replay of failed questions with enhanced variants

### 2. ğŸ”„ Inter-Round Weight Transfer
- Round 1 uses original weights
- Round 2+ automatically loads previous round results
- Supports FSDP distributed weight auto-merging

### 3. ğŸ’¾ Data Persistence
- All training data permanently saved
- Standardized file naming conventions
- Training state recovery support

### 4. ğŸ§© Modular Architecture
- Separated data collection, processing, training, and management modules
- Independent testing and maintenance support
- Complete error handling mechanisms



### ğŸš€ Parallel Strategy
- **ğŸ“Š Data collection**: Multi-GPU parallel with intelligent task allocation
- **âœ… Grading processing**: 32 concurrent Teacher API calls
- **ğŸ“ˆ Question enhancement**: 32 concurrent Teacher2 processing

## ğŸ”§ Troubleshooting

### Common Issues

<details>
<summary>ğŸš¨ Model path not found</summary>

```bash
export SOLVER_MODEL_PATH="/correct/path/to/model"
```
</details>

<details>
<summary>ğŸ–¥ï¸ GPU memory insufficient</summary>

- Check GPU usage: `nvidia-smi`
- Adjust batch_size or reduce parallelism
</details>

<details>
<summary>ğŸ’¾ Checkpoint merge failure</summary>

- Check checkpoint directory permissions
- Confirm FSDP weight files are complete
</details>

<details>
<summary>ğŸ”„ Training interruption recovery</summary>

```bash
# ğŸ¤– Fully automated training recovery
python scripts/auto_trainer.py

# ğŸ® Semi-automated training recovery
python scripts/semi_auto_trainer.py

# ğŸ“Š Check recovery status
python utils/status_checker.py
```
</details>

### ğŸ“ Log Files
- **ğŸ‹ï¸ TRL Training Log**: `/tmp/trl_trainer.log`
- **ğŸ¤– Automated Training Log**: `/tmp/auto_trainer.log`
- **ğŸ“Š Training Output**: Real-time console output
- **ğŸ’¾ State Files**: `{WORKSPACE_DIR}/training_state.json`
- **ğŸ“ˆ Round Progress**: `{WORKSPACE_DIR}/round_XX_progress.json`
- **ğŸ“Š Training Results**: `{WORKSPACE_DIR}/training_results/`
- **ğŸ“‹ Training Summary**: `{WORKSPACE_DIR}/auto_training_summary.json`
- **ğŸ’¾ Checkpoint Files**: `{WORKSPACE_DIR}/checkpoint_round_X.json`

## ğŸ‘¨â€ğŸ’» Development Guide

### Adding New Modules
1. Create new file in appropriate directory
2. Implement standard interfaces and error handling
3. Update corresponding `__init__.py` exports
4. Add unit tests

### Custom Training Strategies
1. Modify `RoundController` configuration
2. Adjust question pool building logic
3. Customize reward calculation functions

### Extending Data Formats
1. Update `StateManager` file naming
2. Modify data save and load logic
3. Ensure backward compatibility

## ğŸ“š Citation

If you use this code in your research, please cite:

```bibtex
@article{socratic2024,
  title={Socratic-Zero: Bootstrapping Reasoning via Data-Free Agent Co-evolution},
  author={Wang, Shaobo and Jiao, Zhengbo and Zhang, Zifan and Peng, Yilang and Ze, Xu and Yang, Boyu and Wang, Wei and Wei, Hu and Zhang, Linfeng},
  journal={arXiv preprint arXiv:2509.24726},
  year={2024},
  url={http://arxiv.org/abs/2509.24726}
}
```

## ğŸ“„ License

This project follows internal use license, for research and development only.

## ğŸ¤ Support

For questions or suggestions, please contact the development team or check project documentation.

---

<div align="center">
  <img src="scripts/method.pdf" alt="Socratic-Zero Framework" width="600"/>
  <br>
  <strong>ğŸ“ Bootstrapping Reasoning Through Socratic Dialogue ğŸ“</strong>
  <br>
  <em>Systematic co-evolutionary training for mathematical reasoning advancement</em>
</div>

